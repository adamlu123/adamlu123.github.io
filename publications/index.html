<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Yadong (Adam) Lu</title> <meta name="author" content="Yadong (Adam) Lu"/> <meta name="description" content="Below is a list of preprints and publications."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://adamlu123.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adamlu123.github.io/"><span class="font-weight-bold">Yadong (Adam)</span> Lu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">Below is a list of preprints and publications.</p> </header> <article> <div class="publications"> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Arxiv</abbr></div> <div id="lu2024omniparserpurevisionbased" class="col-sm-8"> <div class="title">OmniParser for Pure Vision Based GUI Agent</div> <div class="author">Lu, Yadong, Yang, Jianwei, Shen, Yelong, and Awadallah, Ahmed </div> <div class="periodical"> 2024 </div> <div class="links"> <a href="https://arxiv.org/abs/2408.00203" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="ren2024sambasimplehybridstate" class="col-sm-8"> <div class="title">Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling</div> <div class="author">Ren, Liliang, Liu, Yang, Lu, Yadong, Shen, Yelong, Liang, Chen, and Chen, Weizhu </div> <div class="periodical"> <em>ICLR</em> 2024 </div> <div class="links"> <a href="https://arxiv.org/abs/2406.07522" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">TMLR</abbr></div> <div id="zhong2024multiloracompositionimagegeneration" class="col-sm-8"> <div class="title">Multi-LoRA Composition for Image Generation</div> <div class="author">Zhong, Ming, Shen, Yelong, Wang, Shuohang, Lu, Yadong, Jiao, Yizhu, Ouyang, Siru, Yu, Donghan, Han, Jiawei, and Chen, Weizhu </div> <div class="periodical"> <em>TMLR</em> 2024 </div> <div class="links"> <a href="https://arxiv.org/abs/2402.16843" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Arxiv</abbr></div> <div id="hydra_rlhf" class="col-sm-8"> <div class="title">Efficient RLHF: Reducing the Memory Usage of PPO</div> <div class="author">Michael, Santacroce, Yadong, Lu, Han, Yu, Yuanzhi, Li, and Yelong, Shen </div> <div class="periodical"> <em>Arxiv</em> 2023 </div> <div class="links"> <a href="https://arxiv.org/pdf/2309.00754.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="in_context" class="col-sm-8"> <div class="title">In-Context Learning Unlocked for Diffusion Models</div> <div class="author">Wang, Zhendong, Jiang, Yifan, Lu, Yadong, Shen, Yelong, He, Pengcheng, Chen, Weizhu, Wang, Zhangyang, and Zhou, Mingyuan </div> <div class="periodical"> <em>NeurIPS Spotlight</em> 2023 </div> <div class="links"> <a href="https://arxiv.org/abs/2305.01115" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Patent</abbr></div> <div id="https://doi.org/10.48550/arxiv.2202.00723" class="col-sm-8"> <div class="title">Progressive data compression using artificial neural networks</div> <div class="author">Lu, Yadong, Yang, Yang, Zhu, Yinhao, Said, Amir, and Cohen, Taco </div> <div class="periodical"> <em>US Patent</em> 2022 </div> <div class="links"> <a href="https://patents.google.com/patent/US20220237740A1/en" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Patent</abbr></div> <div id="https://doi.org/10.48550/arxiv.2202.00724" class="col-sm-8"> <div class="title">Variable bit rate compression using neural network models</div> <div class="author">Lu, Yadong, Yang, Yang, Zhu, Yinhao, Said, Amir, Pourreza, Reza, and Cohen, Taco </div> <div class="periodical"> <em>US Patent</em> 2022 </div> <div class="links"> <a href="https://patents.google.com/patent/US20220224926A1/en" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">JHEP</abbr></div> <div id="https://doi.org/10.48550/arxiv.2202.00725" class="col-sm-8"> <div class="title">Resolving Extreme Jet Substructure</div> <div class="author">Lu, Yadong, Romero, Alexis, Fenton, Michael James, Whiteson, Daniel, and Baldi, Pierre </div> <div class="periodical"> <em>Journal of High Energy Physics</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/article/10.1007/JHEP08(2022)046" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We study the effectiveness of theoretically-motivated high-level jet observables in the extreme context of jets with a large number of hard sub-jets (up to N=8). Previous studies indicate that high-level observables are powerful, interpretable tools to probe jet substructure for N≤3 hard sub-jets, but that deep neural networks trained on low-level jet constituents match or slightly exceed their performance. We extend this work for up to N=8 hard sub-jets, using deep particle-flow networks (PFNs) and Transformer based networks to estimate a loose upper bound on the classification performance. The high-level jet observables perform well, with an overall classification accuracy of 86.90%, but fall short of the PFN and Transformer models, which reach classification accuracies of 89.19% and 91.27% respectively, suggesting that the constituent networks utilize information not captured by the set of high-level observables. We then identify additional high-level observables which are able to narrow this gap, and utilize LASSO regularization for feature selection to identify and rank the most relevant observables and provide further insights into the learning strategies used by the constituent-based neural networks. The final model contains only 31 high-level observables and is able to match the performance of the PFN and approximate the performance of the Transformer model to within 2%.</p> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Phys Rev.D</abbr></div> <div id="PhysRevD.103.036012" class="col-sm-8"> <div class="title">Sparse autoregressive models for scalable generation of sparse images in particle physics</div> <div class="author">Lu, Yadong, Collado, Julian, Whiteson, Daniel, and Baldi, Pierre </div> <div class="periodical"> <em>Physics Review D</em> Feb 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.aps.org/doi/10.1103/PhysRevD.103.036012" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Generation of simulated data is essential for data analysis in particle physics, but current Monte Carlo methods are very computationally expensive. Deep-learning-based generative models have successfully generated simulated data at lower cost, but struggle when the data are very sparse. We introduce a novel deep sparse autoregressive model (SARM) that explicitly learns the sparseness of the data with a tractable likelihood, making it more stable and interpretable when compared to generative adversarial networks (GANs) and other methods. In two case studies, we compare SARM to a GAN model and a nonsparse autoregressive model. As a quantitative measure of performance, we compute the Wasserstein distance ( W p ) between the distributions of physical quantities calculated on the generated images and on the training images. In the first study, featuring images of jets in which 90% of the pixels are zero valued, SARM produces images with W p scores that are 24%–52% better than the scores obtained with other state-of-the-art generative models. In the second study, on calorimeter images in the vicinity of muons where 98% of the pixels are zero valued, SARM produces images with W p scores that are 66%–68% better. Similar observations made with other metrics confirm the usefulness of SARM for sparse data in particle physics.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICIP</abbr></div> <div id="plonq" class="col-sm-8"> <div class="title">Progressive Neural Image Compression With Nested Quantization And Latent Ordering</div> <div class="author">Lu, Yadong*, Zhu, Yinhao*, Yang, Yang*, Said, Amir, and Cohen, Taco S </div> <div class="periodical"> <em>In IEEE International Conference on Image Processing (ICIP)</em> Feb 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/9506026" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We present PLONQ, a progressive neural image compression scheme which pushes the boundary of variable bitrate compression by allowing quality scalable coding with a single bitstream. In contrast to existing learned variable bitrate solutions which produce separate bitstreams for each quality, it enables easier rate-control and requires less storage. Leveraging the latent scaling based variable bitrate solution, we introduce nested quantization, a method that defines multiple quantization levels with nested quantization grids, and progressively refines all latents from the coarsest to the finest quantization level. To achieve finer progressiveness in between any two quantization levels, latent elements are incrementally refined with an importance ordering defined in the rate-distortion sense. To the best of our knowledge, PLONQ is the first learning-based progressive image coding scheme and it outperforms SPIHT, a well-known wavelet-based progressive image codec.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IJCAI</abbr></div> <div id="ijcai2021-582" class="col-sm-8"> <div class="title">Deep Bucket Elimination</div> <div class="author">Razeghi, Yasaman, Kask, Kalev, Lu, Yadong, Baldi, Pierre, Agarwal, Sakshi, and Dechter, Rina </div> <div class="periodical"> <em>In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</em> Aug 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.24963/ijcai.2021/582" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Bucket Elimination (BE) is a universal inference scheme that can solve most tasks over probabilistic and deterministic graphical models exactly. However, it often requires exponentially high levels of memory (in the induced-width) preventing its execution. In the spirit of exploiting Deep Learning for inference tasks, in this paper, we will use neural networks to approximate BE. The resulting Deep Bucket Elimination (DBE) algorithm is developed for computing the partition function. We provide a proof-of-concept empirically using instances from several different benchmarks, showing that DBE can be a more accurate approximation than current state-of-the-art approaches for approximating BE (eg the mini-bucket schemes), especially when problems are sufficiently hard.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">SCAM</abbr></div> <div id="9610641" class="col-sm-8"> <div class="title">D-REX: Static Detection of Relevant Runtime Exceptions with Location Aware Transformer</div> <div class="author">Farmahinifarahani, Farima*, Lu, Yadong*, Saini, Vaibhav, Baldi, Pierre, and Lopes, Cristina </div> <div class="periodical"> <em>In 2021 IEEE 21st International Working Conference on Source Code Analysis and Manipulation (SCAM)</em> Aug 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9610641" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Runtime exceptions are inevitable parts of software systems. While developers often write exception handling code to avoid the severe outcomes of these exceptions, such code is most effective if accompanied by accurate runtime exception types. Predicting the runtime exceptions that may occur in a program, however, is difficult as the situations that lead to these exceptions are complex. We propose D-REX (Deep Runtime EXception detector), as an approach for predicting runtime exceptions of Java methods based on the static properties of code.The core of D-REX is a machine learning model that leverages the representation learning ability of neural networks to infer a set of signals from code to predict the related runtime exception types. This model, which we call Location Aware Transformer, adapts a state-of-the-art language model, Transformer, to provide accurate predictions for the exception types, as well as interpretable recommendations for the exception prone elements of code. We curate a benchmark dataset of 200,000 Java projects from GitHub to train and evaluate D-REX. Experiments demonstrate that D-REX predicts runtime exception types with 81% of Top 1 accuracy, outperforming multiple non-Transformer baselines by a margin of at least 12%. Furthermore, it can predict the exception prone elements of code with 75% Top 1 precision.</p> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ECCV</abbr></div> <div id="Wang2020DifferentiableJP" class="col-sm-8"> <div class="title">Differentiable Joint Pruning and Quantization for Hardware Efficiency</div> <div class="author">Wang, Yin, Lu, Yadong, and Blankevoort, Tijmen </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV)</em> Aug 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2007.10463" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>We present a differentiable joint pruning and quantization (DJPQ) scheme. We frame neural network compression as a joint gradient-based optimization problem, trading off between model pruning and quantization automatically for hardware efficiency. DJPQ incorporates variational information bottleneck based structured pruning and mixed-bit precision quantization into a single differentiable loss function. In contrast to previous works which consider pruning and quantization separately, our method enables users to find the optimal trade-off between both in a single training procedure. To utilize the method for more efficient hardware inference, we extend DJPQ to integrate structured pruning with power-of-two bit-restricted quantization. We show that DJPQ significantly reduces the number of Bit-Operations (BOPs) for several networks while maintaining the top-1 accuracy of original floating-point models (e.g., 53x BOPs reduction in ResNet18 on ImageNet, 43x in MobileNetV2). Compared to the conventional two-stage approach, which optimizes pruning and quantization independently, our scheme outperforms in terms of both accuracy and BOPs. Even when considering bit-restricted quantization, DJPQ achieves larger compression ratios and better accuracy than the two-stage approach.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE</abbr></div> <div id="8751989" class="col-sm-8"> <div class="title">DCNN-Based Multi-Signal Induction Motor Fault Diagnosis</div> <div class="author">Shao, Siyu, Yan, Ruqiang, Lu, Yadong, Wang, Peng, and Gao, Robert X. </div> <div class="periodical"> <em>IEEE Transactions on Instrumentation and Measurement</em> Aug 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/8751989/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Deep learning (DL) architecture, which exploits multiple hidden layers to learn hierarchical representations automatically from massive input data, presents a promising tool for characterizing fault conditions. This paper proposes a DL-based multi-signal fault diagnosis method that leverages the powerful feature learning ability of a convolutional neural network (CNN) in images. The proposed deep model is able to learn from multiple types of sensor signals simultaneously so that it can achieve robust performance and finally realize accurate induction motor fault recognition. First, the acquired sensor signals are converted to time-frequency distribution (TFD) by wavelet transform. Then, a deep CNN is applied to learning discriminative representations from the TFD images. Since then, a fully connected layer in deep architecture gives the prediction of induction motor condition based on learned features. In order to verify the effectiveness of the designed deep model, experiments are carried out on a machine fault simulator where both vibration and current signals are analyzed. Experimental results indicate that the proposed method outperforms traditional fault diagnosis methods, hence, demonstrating effectiveness in induction motor application. Compared with conventional methods that rely on delicate features extracted by experienced experts, the proposed deep model is able to automatically learn and select suitable features that contribute to accurate fault diagnosis. Compared with single-signal input, the multi-signal model has more accurate and stable performance and overcomes the overfitting problem to some degree.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">AP</abbr></div> <div id="MIAO2020187" class="col-sm-8"> <div class="title">Scalable Bayesian variable selection regression models for count data</div> <div class="author">Miao, Yinsen, Kook, Jeong Hwan, Lu, Yadong, Guindani, Michele, and Vannucci, Marina </div> <div class="periodical"> <em>In Flexible Bayesian Regression Modelling, Academic Press.</em> Aug 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/B9780128158623000159" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Variable selection, also known as feature selection in the machine learning literature, plays an indispensable role in scientific studies. In many research areas with massive data, finding a subset of representative features that best explain the outcome of interest has become a critical component in any researcher’s workflow. In this chapter, we focus on Bayesian variable selection regression models for count data, and specifically on the negative binomial linear regression model and on the Dirichlet-multinomial regression model. We address the variable selection problem via spike-and-slab priors. For posterior inference, we review standard MCMC methods and also investigate computationally more efficient variational inference approaches that use data augmentation techniques and concrete relaxation methods. We investigate performance of the methods via simulation studies and benchmark data sets.</p> </div> </div> </div> </li> </ol> <h2 class="year">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICSE</abbr></div> <div id="10.1109/ICSE.2019.00023" class="col-sm-8"> <div class="title">Towards Automating Precision Studies of Clone Detectors</div> <div class="author">Saini, Vaibhav, Farmahinifarahani, Farima, Lu, Yadong, Yang, Di, Martins, Pedro, Sajnani, Hitesh, Baldi, Pierre, and Lopes, Cristina V. </div> <div class="periodical"> <em>In Proceedings of the 41st International Conference on Software Engineering</em> Aug 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICSE.2019.00023" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Current research in clone detection suffers from poor ecosystems for evaluating precision of clone detection tools. Corpora of labeled clones are scarce and incomplete, making evaluation labor intensive and idiosyncratic, and limiting inter-tool comparison. Precision-assessment tools are simply lacking.We present a semiautomated approach to facilitate precision studies of clone detection tools. The approach merges automatic mechanisms of clone classification with manual validation of clone pairs. We demonstrate that the proposed automatic approach has a very high precision and it significantly reduces the number of clone pairs that need human validation during precision experiments. Moreover, we aggregate the individual effort of multiple teams into a single evolving dataset of labeled clone pairs, creating an important asset for software clone research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS-ML4S</abbr></div> <div id="Lu2019SparseIG" class="col-sm-8"> <div class="title">Sparse Image Generation with Decoupled Generative Models</div> <div class="author">Lu, Yadong, Collad, Julian, Bauer, Kevin, Whiteson, Daniel, and Baldi, Pierre </div> <div class="periodical"> <em>In </em> Aug 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ml4physicalsciences.github.io/2019/files/NeurIPS_ML4PS_2019_161.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Learning to generate very sparse images with deep generative models has been a challenging problem due to issues such as mode collapse and sparse gradient signals. In this work, we propose a novel model combining a neural network generator with an explicit mixture model, which induces sparsity with a learnable Dirac delta mass at zero instead of using rectified linear units at the output. Our model decouples the sparsity level and the non-zero distribution of each pixel in the data while fitting both of them simultaneously by minimizing the model’s entropy. We demonstrate both theoretically and empirically that the model is able to learn a rich distribution of a sparse muon image dataset while maintaining desired physical properties such as isolation.</p> </div> </div> </div> </li> </ol> <h2 class="year">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ESEC/FSE</abbr></div> <div id="10.1145/3236024.3236026" class="col-sm-8"> <div class="title">Oreo: Detection of Clones in the Twilight Zone</div> <div class="author">Saini, Vaibhav, Farmahinifarahani, Farima, Lu, Yadong, Baldi, Pierre, and Lopes, Cristina V. </div> <div class="periodical"> <em>In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em> Aug 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3236024.3236026" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>Source code clones are categorized into four types of increasing difficulty of detection, ranging from purely textual (Type-1) to purely semantic (Type-4). Most clone detectors reported in the literature work well up to Type-3, which accounts for syntactic differences. In between Type-3 and Type-4, however, there lies a spectrum of clones that, although still exhibiting some syntactic similarities, are extremely hard to detect – the Twilight Zone. Most clone detectors reported in the literature fail to operate in this zone. We present Oreo, a novel approach to source code clone detection that not only detects Type-1 to Type-3 clones accurately, but is also capable of detecting harder-to-detect clones in the Twilight Zone. Oreo is built using a combination of machine learning, information retrieval, and software metrics. We evaluate the recall of Oreo on BigCloneBench, and perform manual evaluation for precision. Oreo has both high recall and precision. More importantly, it pushes the boundary in detection of clones with moderate to weak syntactic similarity in a scalable manner</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Yadong (Adam) Lu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>